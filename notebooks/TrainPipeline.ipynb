{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import CachingImagesDataset\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import densenet201, squeezenet1_0\n",
    "from torchvision import transforms\n",
    "from torchnet import meter\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "from os.path import basename, join, exists\n",
    "from os import listdir, makedirs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet201Model(torch.nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.features = densenet201(pretrained=True).features\n",
    "        last_module = list(self.features.modules())[-1]\n",
    "        self.classifier = torch.nn.Linear(last_module.num_features, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        images = x['image']\n",
    "        features = self.features(images)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SqueezeNetModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = squeezenet1_0(pretrained=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        images = x['image']\n",
    "        result = self.model.forward(images)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def prepare_model(model_params):\n",
    "    model = DenseNet201Model()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# prepare_for_training - + load hparams, - train/valid loggers\n",
    "# prepare_model - + prepare model and optimizer by params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training(hparams):\n",
    "    if 'model_params' not in hparams:\n",
    "        raise Exception('You must add model params to hparams')\n",
    "    \n",
    "    model = prepare_model(hparams['model_params'])\n",
    "    \n",
    "    if 'criterion_params' not in hparams or \\\n",
    "        'criterion' not in hparams['criterion_params']:\n",
    "        raise Exception('You must add criterion params to hparams')\n",
    "    \n",
    "    criterion_params = hparams['criterion_params']\n",
    "    criterion_name = criterion_params.pop('criterion')\n",
    "    criterion = torch.nn.__dict__[criterion_name](**criterion_params)\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    if 'optimizer_params' not in hparams or \\\n",
    "        'optimizer' not in hparams['optimizer_params']:\n",
    "        raise Exception('You must add optimizer params to hparams')\n",
    "    \n",
    "    optimizer_params = hparams['optimizer_params']\n",
    "    optimizer_name = optimizer_params.pop('optimizer')\n",
    "    optimizer = torch.optim.__dict__[optimizer_name](\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        **optimizer_params\n",
    "    )\n",
    "    \n",
    "    if 'scheduler_params' in hparams:\n",
    "        scheduler_params = hparams['scheduler_params']\n",
    "        if 'scheduler' not in scheduler_params:\n",
    "            raise Exception('If you provided scheduler params you also must add scheduler name')\n",
    "        scheduler_name = scheduler_params.pop('scheduler')\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.__dict__[scheduler_name](\n",
    "            optimizer, **scheduler_params\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LABEL_NAMES = ['MEL', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC', 'NV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_fold_split(df, column, folds_seed, n_folds):\n",
    "    df_tmp = []\n",
    "    labels = shuffle(sorted(df[column].unique()), random_state=folds_seed)\n",
    "    for i, fold_labels in enumerate(np.array_split(labels, n_folds)):\n",
    "        df_label = df[df[column].isin(fold_labels)]\n",
    "        df_label['fold'] = i\n",
    "        df_tmp.append(df_label)\n",
    "    df = pd.concat(df_tmp)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_labels(path):\n",
    "    labels = pd.read_csv(path,\n",
    "                        dtype={**{'image': str},\n",
    "                               **{label: int for label in TARGET_LABEL_NAMES}})\n",
    "    labels.image += '.jpg'\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def prepare_data_loaders(hparams):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    \n",
    "    if 'data_params' not in hparams:\n",
    "        raise Exception('You must provide data params in hparams')\n",
    "    \n",
    "    data_params = hparams['data_params']\n",
    "    if 'transforms' not in data_params or not isinstance(data_params['transforms'], list):\n",
    "        raise Exception('You must add transforms list into hparams.data_params')\n",
    "    \n",
    "    transforms_list = []\n",
    "    for transform_info in data_params['transforms']:\n",
    "        transform_name = transform_info['name']\n",
    "        transform_params = transform_info['params']\n",
    "        if transform_params is not None:\n",
    "            transform = transforms.__dict__[transform_name](**transform_params)\n",
    "        else:\n",
    "            transform = transforms.__dict__[transform_name]()\n",
    "        transforms_list.append(transform)\n",
    "    transform = transforms.Compose(transforms_list)\n",
    "        \n",
    "    if 'labels_path' not in data_params:\n",
    "        raise Exception('You must add labels_path into hparams')\n",
    "    \n",
    "    if 'n_folds' not in data_params or 'train_folds' not in data_params or \\\n",
    "        'folds_split_column' not in data_params or 'folds_seed' not in data_params:\n",
    "        raise Exception('You must add n_folds, train_folds, folds_split_column' \\\n",
    "                        'and folds_seed into hparams.data_params')\n",
    "    \n",
    "    labels = read_labels(data_params['labels_path'])\n",
    "    labels = column_fold_split(labels, data_params['folds_split_column'],\n",
    "                      data_params['folds_seed'], data_params['n_folds'])\n",
    "    \n",
    "    if 'images_path' not in data_params:\n",
    "        raise Exception('You must add images_path into hparams.data_params')\n",
    "    \n",
    "    train_folds = list(map(int, data_params['train_folds'].split(\",\")))\n",
    "    train_labels = labels[labels['fold'].isin(train_folds)]\n",
    "    valid_labels = labels[~labels['fold'].isin(train_folds)]\n",
    "    \n",
    "    train_labels = train_labels.reset_index().drop('index', axis=1)\n",
    "    valid_labels = valid_labels.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    train_dataset = CachingImagesDataset(train_labels, data_params['images_path'],\n",
    "                                         TARGET_LABEL_NAMES, transform=transform,\n",
    "                                         image_filename_column=data_params['folds_split_column'])\n",
    "    valid_transform = transforms.Compose(list(filter(\n",
    "        lambda t: isinstance(t, transforms.Resize) or \\\n",
    "            isinstance(t, transforms.ToTensor) or \\\n",
    "            isinstance(t, transforms.Normalize), transforms_list)))\n",
    "    valid_dataset = CachingImagesDataset(valid_labels, data_params['images_path'],\n",
    "                                         TARGET_LABEL_NAMES, transform=valid_transform,\n",
    "                                         image_filename_column=data_params['folds_split_column'])\n",
    "    \n",
    "    if 'training_params' not in hparams or 'batch_size' not in hparams['training_params']:\n",
    "        raise Exception('You must add training_params with batch_size specified in hparams')\n",
    "    training_params = hparams['training_params']\n",
    "    \n",
    "    n_workers = data_params['n_workers'] if 'n_workers' in data_params else 0\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'],\n",
    "                              shuffle=True, num_workers=n_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=training_params['batch_size'],\n",
    "                              shuffle=False, num_workers=n_workers,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_from_metric(metric_value):\n",
    "    if isinstance(metric_value, (int, float)):\n",
    "        return metric_value\n",
    "    else:\n",
    "        metric_value = metric_value.value()\n",
    "        if isinstance(metric_value, tuple):\n",
    "            metric_value = metric_value[0]\n",
    "        return metric_value\n",
    "\n",
    "\n",
    "def run_train_val_loader(epoch, loader, mode, model, criterion, optimizer):\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    epoch_metrics = {\n",
    "        \"loss\": meter.AverageValueMeter(),\n",
    "#         \"auc\": meter.AUCMeter()\n",
    "    }\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        if i > 5:\n",
    "            break\n",
    "        \n",
    "        target = batch.pop('target')\n",
    "        batch_size = len(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_var = {\n",
    "                key: torch.autograd.Variable(value.cuda(async=True), requires_grad=False)\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "        else:\n",
    "            input_var = {\n",
    "                key: torch.autograd.Variable(value, requires_grad=False)\n",
    "                for key, value in batch.items()\n",
    "            }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda(async=True).type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            target = target.type(torch.LongTensor)\n",
    "        target_var = torch.autograd.Variable(target, requires_grad=False)\n",
    "        \n",
    "        output = model.forward(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        \n",
    "        epoch_metrics['loss'].add(float(loss.data.cpu().numpy()))\n",
    "#         epoch_metrics[\"auc\"].add(output.data, target)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_metrics['batch_size'] = batch_size\n",
    "    \n",
    "    out_metrics = {key: get_val_from_metric(value) for key, value in epoch_metrics.items()}\n",
    "    epoch_metrics_str = \"\\t\".join([\n",
    "        \"{key} {value:.4f}\".format(key=key, value=value)\n",
    "        for key, value in sorted(out_metrics.items())])\n",
    "    print(\"{epoch} * Epoch ({mode}): \".format(epoch=epoch, mode=mode) + epoch_metrics_str)\n",
    "    \n",
    "    return out_metrics\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, logdir):\n",
    "    if not exists(logdir):\n",
    "        makedirs(logdir)\n",
    "    \n",
    "    filename = \"{}/checkpoint.pth.tar\".format(logdir)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, '{}/checkpoint.best.pth.tar'.format(logdir))\n",
    "\n",
    "\n",
    "def run_train(hparams, args):\n",
    "    model, criterion, optimizer, scheduler = prepare_training(hparams)\n",
    "    \n",
    "    best_loss = int(1e10)\n",
    "    best_metrics = None\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_loss = checkpoint['best_loss']\n",
    "            best_metrics = checkpoint['best_metrics']\n",
    "\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            raise Exception(\"no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        # speed up\n",
    "        cudnn.benchmark = True\n",
    "    else:\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "    train_loader, valid_loader = prepare_data_loaders(hparams)\n",
    "    \n",
    "    if 'training_params' not in hparams:\n",
    "        raise Exception('You must provide training_params in hparams')\n",
    "    \n",
    "    training_params = hparams['training_params']\n",
    "    if 'epochs' not in training_params or 'batch_size' not in training_params:\n",
    "        raise Exception('You must add epochs and batch_size parameters into hparams')\n",
    "    \n",
    "    for epoch in range(start_epoch, training_params['epochs']):\n",
    "        run_train_val_loader(epoch, train_loader, 'train', model, criterion, optimizer)\n",
    "        epoch_val_metrics = run_train_val_loader(epoch, valid_loader, 'valid', model, criterion, optimizer)\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(epoch_val_metrics[\"loss\"])\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = epoch_val_metrics[\"loss\"] < best_loss\n",
    "        best_loss = min(epoch_val_metrics[\"loss\"], best_loss)\n",
    "        best_metrics = epoch_val_metrics if is_best else best_metrics\n",
    "        best_metrics = {\n",
    "            key: value for key, value in best_metrics.items()\n",
    "            if isinstance(value, float)}\n",
    "        save_checkpoint({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"best_metrics\": epoch_val_metrics,\n",
    "            \"model\": model.module,\n",
    "            \"model_state_dict\": model.module.state_dict(),\n",
    "            \"optimizer\": optimizer,\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, is_best, logdir=args.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.resume = False\n",
    "args.logdir = './log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 * Epoch (train): batch_size 1.0000\tloss 2.2909\n",
      "0 * Epoch (valid): batch_size 1.0000\tloss 2.1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladvin/anaconda3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DenseNet201Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 * Epoch (train): batch_size 1.0000\tloss 1.8393\n",
      "1 * Epoch (valid): batch_size 1.0000\tloss 1.9535\n",
      "2 * Epoch (train): batch_size 1.0000\tloss 1.4761\n",
      "2 * Epoch (valid): batch_size 1.0000\tloss 1.7137\n"
     ]
    }
   ],
   "source": [
    "hparams_path = '../hparams.json'\n",
    "with open(hparams_path, 'r') as f:\n",
    "    hparams = json.load(f, object_pairs_hook=OrderedDict)\n",
    "\n",
    "run_train(hparams, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
