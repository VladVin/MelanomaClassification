{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import CachingImagesDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import densenet201\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "from os.path import basename, join\n",
    "from os import listdir, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_params):\n",
    "    model = densenet201(pretrained=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# prepare_for_training - + load hparams, - train/valid loggers\n",
    "# prepare_model - + prepare model and optimizer by params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_path = '../hparams.json'\n",
    "with open(hparams_path, 'r') as f:\n",
    "    hparams = json.load(f, object_pairs_hook=OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training(hparams):\n",
    "    if 'model_params' not in hparams:\n",
    "        raise Exception('You must add model params to hparams')\n",
    "    \n",
    "    model = prepare_model(hparams['model_params'])\n",
    "    \n",
    "    if 'criterion_params' not in hparams or \\\n",
    "        'criterion' not in hparams['criterion_params']:\n",
    "        raise Exception('You must add criterion params to hparams')\n",
    "    \n",
    "    criterion_params = hparams['criterion_params']\n",
    "    criterion = torch.nn.__dict__[criterion_params['criterion']](**criterion_params)\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    if 'optimizer_params' not in hparams or \\\n",
    "        'optimizer' not in hparams['optimizer_params']:\n",
    "        raise Exception('You must add optimizer params to hparams')\n",
    "    \n",
    "    optimizer_params = hparams['optimizer_params']\n",
    "    optimizer = torch.optim.__dict__[optimizer_params['optimizer']](\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        **optimizer_params\n",
    "    )\n",
    "    \n",
    "    if 'scheduler_params' in hparams:\n",
    "        scheduler_params = hparams['scheduler_params']\n",
    "        if 'scheduler' not in scheduler_params:\n",
    "            raise Exception('If you provided scheduler params you also must add scheduler name')\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.__dict__[scheduler_params['scheduler']](\n",
    "            optimizer, **scheduler_params\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    return model, criterion, optimzer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LABEL_NAMES = ['MEL', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC', 'NV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_fold_split(df, column, folds_seed, n_folds):\n",
    "    df_tmp = []\n",
    "    labels = shuffle(sorted(df[column].unique()), random_state=folds_seed)\n",
    "    for i, fold_labels in enumerate(np.array_split(labels, n_folds)):\n",
    "        df_label = df[df[column].isin(fold_labels)]\n",
    "        df_label['fold'] = i\n",
    "        df_tmp.append(df_label)\n",
    "    df = pd.concat(df_tmp)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_labels(path):\n",
    "    labels = pd.read_csv(path,\n",
    "                        dtype={**{'image': str},\n",
    "                               **{label: int for label in TARGET_LABEL_NAMES}})\n",
    "    labels.image += '.jpg'\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def prepare_data_loaders(hparams):\n",
    "    if 'data_params' not in hparams:\n",
    "        raise Exception('You must provide data params in hparams')\n",
    "    \n",
    "    data_params = hparams['data_params']\n",
    "    if 'transforms' not in data_params or not isinstance(data_params['transforms'], list):\n",
    "        raise Exception('You must add transforms list into hparams.data_params')\n",
    "    \n",
    "    transforms_list = []\n",
    "    for transform_info in data_params['transforms']:\n",
    "        transform_name = transform_info['name']\n",
    "        transform_params = transform_info['params']\n",
    "        if transform_params is not None:\n",
    "            transform = transforms.__dict__[transform_name](**transform_params)\n",
    "        else:\n",
    "            transform = transforms.__dict__[transform_name]()\n",
    "        transforms_list.append(transform)\n",
    "    transform = transforms.Compose(transforms_list)\n",
    "        \n",
    "    if 'labels_path' not in data_params:\n",
    "        raise Exception('You must add labels_path into hparams')\n",
    "    \n",
    "    if 'n_folds' not in data_params or 'train_folds' not in data_params or \\\n",
    "        'folds_split_column' not in data_params or 'folds_seed' not in data_params:\n",
    "        raise Exception('You must add n_folds, train_folds, folds_split_column' \\\n",
    "                        'and folds_seed into hparams.data_params')\n",
    "    \n",
    "    labels = read_labels(data_params['labels_path'])\n",
    "    labels = column_fold_split(labels, data_params['folds_split_column'],\n",
    "                      data_params['folds_seed'], data_params['n_folds'])\n",
    "    \n",
    "    if 'images_path' not in data_params:\n",
    "        raise Exception('You must add images_path into hparams.data_params')\n",
    "    \n",
    "    train_folds = list(map(int, data_params['train_folds'].split(\",\")))\n",
    "    train_labels = labels[labels['fold'].isin(train_folds)]\n",
    "    valid_labels = labels[~labels['fold'].isin(train_folds)]\n",
    "    \n",
    "    train_labels = train_labels.reset_index().drop('index', axis=1)\n",
    "    valid_labels = valid_labels.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    train_dataset = CachingImagesDataset(train_labels, data_params['images_path'],\n",
    "                                         TARGET_LABEL_NAMES, transform=transform,\n",
    "                                         image_filename_column=data_params['folds_split_column'])\n",
    "    valid_dataset = CachingImagesDataset(valid_labels, data_params['images_path'],\n",
    "                                         TARGET_LABEL_NAMES, transform=transforms.ToTensor(),\n",
    "                                         image_filename_column=data_params['folds_split_column'])\n",
    "    \n",
    "    if 'training_params' not in hparams or 'batch_size' not in hparams['training_params']:\n",
    "        raise Exception('You must add training_params with batch_size specified in hparams')\n",
    "    training_params = hparams['training_params']\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'],\n",
    "                              shuffle=True, num_workers=-1,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=training_params['batch_size'],\n",
    "                              shuffle=False, num_workers=-1,\n",
    "                              pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = prepare_data_loaders(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(hparams, args):\n",
    "    best_loss = int(1e10)\n",
    "    best_metrics = None\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
